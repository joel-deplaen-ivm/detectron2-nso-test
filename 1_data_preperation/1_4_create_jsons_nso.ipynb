{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38c314c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import geojson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from osgeo import gdal\n",
    "import sys\n",
    "sys.path.append('/scistor/ivm/ako268/nso_repo/detectron2-nso-test/1_data_preperation')\n",
    "from utils.functions import grab_certain_file\n",
    "\n",
    "root = \"/scistor/ivm/project/NSO/timeseries_50_baseline\"\n",
    "big_tiles_folder = root + \"/big_tiles\"\n",
    "small_tiles_folder = root + \"/small_tiles\"\n",
    "annotation_path = root + \"/annotations_shp/fixed_substations_NL_annotations.shp\"\n",
    "geojson_folder = root + \"/geojsons\"\n",
    "json_folder = root + \"/jsons\"\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "413cdcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2, seed=13):\n",
    "    assert(train_ratio + val_ratio + test_ratio == 1.0)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Calculate split sizes\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    val_size = int(len(data) * val_ratio)\n",
    "\n",
    "    # Split the data\n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:(train_size + val_size)]\n",
    "    test_data = data[(train_size + val_size):]\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23be78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function needs the filestructure to be as follows.\n",
    "# Top level\n",
    "# - root\n",
    "# Sub level: The root folder should contain\n",
    "# - \"small_tiles\" folder containing the tiled .tif files\n",
    "# - \"geojsons\" folder containing the geojsons annotations\n",
    "# - \"jsons\" folder being empty. This is where the created json will be stored\n",
    "\n",
    "def create_json(set_list, set_type, root=root):\n",
    "    assert set_type in ['train', 'val', 'test'], \"Invalid set_type. Expected 'train', 'val', or 'test'\"\n",
    "    \n",
    "    small_tiles_folder=root+\"/small_tiles\"\n",
    "    geojsons_folder=root+\"/geojsons\"\n",
    "    jsons_folder=root+\"/jsons\"\n",
    "\n",
    "    print('start processing', len(set_list), 'small tif files for', set_type, 'set')\n",
    "    set_dict = {}\n",
    "    for file in tqdm(set_list, desc=f\"Creating JSONs for Detectron2 on {set_type}\", ncols=150, bar_format=\"{l_bar}{bar:10}{r_bar}\"):\n",
    "        file_path = os.path.join(small_tiles_folder, file)\n",
    "        img_id = file.split(\".tif\")[0]\n",
    "        geojson_path = os.path.join(geojsons_folder, f\"{img_id}.geojson\")\n",
    "\n",
    "        if os.path.exists(geojson_path):\n",
    "            with open(geojson_path) as f:\n",
    "                gj = geojson.load(f)\n",
    "\n",
    "            regions = {}\n",
    "            num_buildings = len(gj[\"features\"])\n",
    "            if num_buildings > 0:\n",
    "                gdal_image = gdal.Open(file_path)\n",
    "                pixel_width, pixel_height = gdal_image.GetGeoTransform()[1], gdal_image.GetGeoTransform()[5]\n",
    "                originX, originY = gdal_image.GetGeoTransform()[0], gdal_image.GetGeoTransform()[3]\n",
    "\n",
    "                for i in range(num_buildings):\n",
    "                    points = gj[\"features\"][i][\"geometry\"][\"coordinates\"][0]\n",
    "\n",
    "                    if len(points) == 1:\n",
    "                        points = points[0]\n",
    "\n",
    "                    all_points_x, all_points_y = [], []\n",
    "                    for j in range(len(points)):\n",
    "                        all_points_x.append(int(round((points[j][0] - originX) / pixel_width)))\n",
    "                        all_points_y.append(int(round((points[j][1] - originY) / pixel_height)))\n",
    "\n",
    "                    regions[str(i)] = {\"shape_attributes\":\n",
    "                                           {\"name\": \"polygon\",\n",
    "                                            \"all_points_x\": all_points_x,\n",
    "                                            \"all_points_y\": all_points_y,\n",
    "                                            \"category\": 0\n",
    "                                           },\n",
    "                                       \"region_attributes\": {}\n",
    "                                      }\n",
    "\n",
    "            dictionary = {\"file_ref\": '',\n",
    "                          \"size\": os.path.getsize(file_path),\n",
    "                          \"filename\": file.replace(\".tif\", \".png\"),\n",
    "                          \"base64_img_data\": '',\n",
    "                          \"file_attributes\": {},\n",
    "                          \"regions\": regions,\n",
    "                          \"origin_x\": originX,\n",
    "                          \"origin_y\": originY\n",
    "                         }\n",
    "            set_dict[file.replace(\".tif\", \".png\")] = dictionary\n",
    "\n",
    "    print('found a total of', len(set_dict), 'substation annotations for', set_type, 'set')\n",
    "    \n",
    "    with open(f\"{jsons_folder}/via_region_{set_type}.json\", \"w\") as f:\n",
    "        json.dump(set_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09a3ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.get_device_name(0)\n",
    "    device_count = torch.cuda.device_count()\n",
    "    compute_capability = torch.cuda.get_device_capability(0)\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Number of GPUs: {device_count}\")\n",
    "    print(f\"Compute Capability: {compute_capability}\")\n",
    "    print(f\"Total Memory: {total_memory} bytes\")\n",
    "else:\n",
    "    print(\"No GPU available.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9d9e91b",
   "metadata": {},
   "source": [
    "'''\n",
    "Read NSO tiles and annotations geojsons, convert lat/lon of tile to pixel coordinates and save pixel coordinates into\n",
    ".json file If more than on .json file can be saved as one via_regions.json.\n",
    "Source:  https://github.com/rl02898/detectron2-spacenet. JDP Edits:some and saving json with origins of tile in jsons to allow stiching back\n",
    "of the tiles.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c3ede9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('/scistor/ivm/ako268/nso_repo/detectron2-nso-test/1_data_preperation/prepare_imagery/datafiles/cleanbaseline_input.csv')\n",
    "len(list(set(df['coordinate_str'])))\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Split the coordinates into 60% train, 20% val, 20% test\n",
    "df_train, df_temp = train_test_split(df, test_size=0.4, random_state=13)\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=13)\n",
    "\n",
    "print(sum(df_train['wms_name'].isin(df_val['wms_name'])))\n",
    "print(sum(df_train['wms_name'].isin(df_test['wms_name'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2022b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "big_tiles_folder = root + \"/big_tiles\"\n",
    "# small_tiles_folder = root + \"small_tiles\"\n",
    "# small_tiles_png = root + \"small_tiles_png\"\n",
    "\n",
    "files = os.listdir(big_tiles_folder)\n",
    "tile_names = [file.strip('.tif') for file in files if file.endswith(\".tif\")]\n",
    "# create 60% train, 20% validate, 20% test\n",
    "train, val, test = split_data(tile_names)\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4cd89ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of subs in train 26\n",
      "number of subs in val 11\n",
      "number of subs in test 10\n"
     ]
    }
   ],
   "source": [
    "# note: these numbers are based on the input. In practice, some subs may fall off because they are on the edge. Also, there may be subs on the tile that we did not select\n",
    "\n",
    "train_wms = [name.split('_SV_RD_8bit_RGB_50cm_')[0] for name in train]\n",
    "print('number of subs in train', sum(df['wms_name'].isin(train_wms)))\n",
    "\n",
    "val_wms = [name.split('_SV_RD_8bit_RGB_50cm_')[0] for name in val]\n",
    "print('number of subs in val', sum(df['wms_name'].isin(val_wms)))\n",
    "\n",
    "test_wms = [name.split('_SV_RD_8bit_RGB_50cm_')[0] for name in test]\n",
    "print('number of subs in test', sum(df['wms_name'].isin(test_wms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f49ce02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5958117354343769\n"
     ]
    }
   ],
   "source": [
    "all_files = grab_certain_file(\".tif\", small_tiles_folder)\n",
    "\n",
    "train_set = [file for file in all_files for name in train if name in file]\n",
    "val_set = [file for file in all_files for name in val if name in file]\n",
    "test_set = [file for file in all_files for name in test if name in file]\n",
    "\n",
    "print(len(train_set)/(len(val_set)+len(test_set)+len(train_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a5787a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small tiles folder exists True\n",
      "geojsons folder exists True\n",
      "jsons folder exists True\n",
      "start processing 14368 small tif files for train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating JSONs for Detectron2 on train: 100%|██████████| 14368/14368 [00:00<00:00, 79384.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found a total of 62 substation annotations for train set\n",
      "small tiles folder exists True\n",
      "geojsons folder exists True\n",
      "jsons folder exists True\n",
      "start processing 4513 small tif files for val set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating JSONs for Detectron2 on val: 100%|██████████| 4513/4513 [00:00<00:00, 9656.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found a total of 21 substation annotations for val set\n",
      "small tiles folder exists True\n",
      "geojsons folder exists True\n",
      "jsons folder exists True\n",
      "start processing 5234 small tif files for test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating JSONs for Detectron2 on test: 100%|██████████| 5234/5234 [00:00<00:00, 10768.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found a total of 17 substation annotations for test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_json(train_set, 'train')\n",
    "create_json(val_set, 'val')\n",
    "create_json(test_set, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5db6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons_folder=root+\"/jsons\"\n",
    "\n",
    "import json\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = jsons_folder + \"/via_region_test.json\"\n",
    "\n",
    "# Open the JSON file and read its contents\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Now you can access the data from the JSON file\n",
    "# For example, let's say your JSON file contains a list of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fadf109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_ref': '',\n",
       " 'size': 2791197,\n",
       " 'filename': '20220303_101829_SV1-01_SV_RD_8bit_RGB_50cm_Helmond_20000_23000.png',\n",
       " 'base64_img_data': '',\n",
       " 'file_attributes': {},\n",
       " 'regions': {'0': {'shape_attributes': {'name': 'polygon',\n",
       "    'all_points_x': [990,\n",
       "     953,\n",
       "     927,\n",
       "     943,\n",
       "     959,\n",
       "     953,\n",
       "     919,\n",
       "     901,\n",
       "     933,\n",
       "     922,\n",
       "     888,\n",
       "     872,\n",
       "     901,\n",
       "     895,\n",
       "     914,\n",
       "     901,\n",
       "     831,\n",
       "     946,\n",
       "     999,\n",
       "     990],\n",
       "    'all_points_y': [383,\n",
       "     366,\n",
       "     432,\n",
       "     438,\n",
       "     445,\n",
       "     471,\n",
       "     457,\n",
       "     498,\n",
       "     513,\n",
       "     535,\n",
       "     524,\n",
       "     563,\n",
       "     582,\n",
       "     602,\n",
       "     610,\n",
       "     642,\n",
       "     607,\n",
       "     333,\n",
       "     358,\n",
       "     383],\n",
       "    'category': 0},\n",
       "   'region_attributes': {}}},\n",
       " 'origin_x': 174196.0,\n",
       " 'origin_y': 391562.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = list(json_data.keys())[0]\n",
    "json_data[key].keys()\n",
    "json_data[key] # change to save sub info!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18dc70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
